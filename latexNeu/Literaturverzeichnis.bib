% This file was created with JabRef 2.5.
% Encoding: UTF-8
@ARTICLE{introToCNN,
 	author    = {Keiron O'Shea and Ryan Nash},
	year = {2015},
	month = {11},
	pages = {},
	title = {An Introduction to Convolutional Neural Networks},
	journal = {ArXiv e-prints}
}
@ARTICLE{surveyOfDeepLearing,
	author={L. {Jiao} and F. {Zhang} and F. {Liu} and S. {Yang} and L. {Li} and Z. {Feng} and R. {Qu}},
	journal={IEEE Access}, 
	title={A Survey of Deep Learning-Based Object Detection}, 
	year={2019},
	volume={7},
	number={},
	pages={128837-128868},
}

	
@misc{spatialMapping,
	author = "Microsoft",
	title = {Spatial Mapping},
	year = "2018",
	url = {https://docs.microsoft.com/de-de/windows/mixed-reality/spatial-mapping},
	note = "[Online; Stand 17. Septmber 2020]"
}
	
@misc{spatialMappingUnity,
	author ="Microsoft",
	title = {Räumliche Zuordnung in Unity},
	year = "2018",
	url = {https://docs.microsoft.com/de-de/windows/mixed-reality/spatial-mapping-in-unity},
	note = "[Online; Stand 17. Septmber 2020]"
}
	
@INPROCEEDINGS{cNNforClass,
	author={N. {Jmour} and S. {Zayen} and A. {Abdelkrim}},
	booktitle={2018 International Conference on Advanced Systems and Electric Technologies (IC ASET)}, 
	title={Convolutional neural networks for image classification}, 
	year={2018},
	volume={},
	number={},
	pages={397-402}
}
	
@misc{getAzure,
	author = "Microsoft",
	title = {Microsoft Azure Computer Vsion},
	url = {https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/},
	note = "[Online; Stand 17. September 2020]"
}
@misc{whatIsAzure,
	author ="Microsoft",
	title = {What is Computer Vision},
	year = "2020",
	url = {https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home},
	note = "[Online; Stand 17. September 2020]"
}
@misc{objDetectAzure,
	author = "Microsoft",
	year = "2019",
	title = {Detect common objects in images},
	url = {https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-object-detection},
	note = "[Online; Stand 17. September 2020]"
}
	
@misc{pytorch,
	author = "pytorch",
	title = {From research to production},
	url = {https://pytorch.org/},
	note = "[Online; Stand 17. September 2020]"
}
@misc{Azure302Doc,
	author ="Microsoft",
	year = "2018",
	title = {Mr und Azure 302 Maschinelles Sehen},
	url = {https://docs.microsoft.com/de-de/windows/mixed-reality/mr-azure-302},
	note = "[Online; Stand 17. September 2020]"
}
@misc{Azure302bDoc,
	author = "Micosoft",
	year = "2018",
	title = {Mr und Azure 302b benutzerdefinierte Vision},
	url = {https://docs.microsoft.com/de-de/windows/mixed-reality/mr-azure-302b},
	note = "[Online; Stand 17. September 2020]"
}
@INPROCEEDINGS{cars,
	author={X. {Chen} and H. {Ma} and J. {Wan} and B. {Li} and T. {Xia}},
	booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Multi-view 3D Object Detection Network for Autonomous Driving}, 
	year={2017},
	volume={},
	number={},
	pages={6526-6534},
}
@misc{mlluminfeatures,
	author = "MagicLeap",
	year ="2020",
	title = {Magic Leap Features},
	url = {https://developer.magicleap.com/en-us/learn/guides/magic-leap-features},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlluminOS,
	author = "MagicLeap",
	year = "2019",
	title = {Lumin OS Overview},
	url = {https://developer.magicleap.com/en-us/learn/guides/lumin-os-overview},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlluminworldreconstruktion,
	author = "MagicLeap",
	year ="2019",
	title = {World Reconstruktion},
	url = {https://developer.magicleap.com/en-us/learn/guides/world-reconstruction-overview-landing},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlmeshingunity,
	author = "MagicLeap",
	year="2020",
	title = {1.4 Spatial Meshing - Unity},
	url = {https://developer.magicleap.com/en-us/learn/guides/meshing-in-unity},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlappsecurity,
	author = "Magic Leap",
	year ="2018",
	title = {App Security},
	url = {https://developer.magicleap.com/en-us/learn/guides/application-security-overview},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlofficialsalespitch,
	author = "MagicLeap",
	year = "2018",
	title = {magic leap 1},
	url = {https://www.magicleap.com/en-us/magic-leap-1},
	note = "[Online; Stand 18. September 2020]"
}
@misc{mlglossary,
	author = "MagicLeap",
	year = "2020",
	title = {Glossary and Usage},
	url = {https://developer.magicleap.com/en-us/learn/guides/glossary},
	note = "[Online; Stand 18. September 2020]"
}

@misc{unitycameratoworldmatrix,
	author = "Unity ",
	year = "2020",
	title = {Camera.cameraToWorldMatrix},
	url = {https://docs.unity3d.com/ScriptReference/Camera-cameraToWorldMatrix.html},
	note = "[Online; Stand 18. September 2020]"
}
@misc{unitymultiplyoint,
	author = "Unity",
	year ="2020",
	title = {Matrix4x4.MultiplyPoint},
	url = {https://docs.unity3d.com/ScriptReference/Matrix4x4.MultiplyPoint.html},
	note = "[Online; Stand 18. September 2020]"
}
@misc{azureobjdetec,
	author = "Microsoft",
	title = {Erkennen von alltäglichen Objekten in Bildern},
	year="2019",
	url = {https://docs.microsoft.com/de-de/azure/cognitive-services/computer-vision/concept-object-detection},
	note = "[Online; Stand 24. September 2020]"
}	
@misc{fromjson,
	author ="Unity",
	year = "2020",
	title = {JsonUtility.FromJson},
	url = {https://docs.unity3d.com/ScriptReference/JsonUtility.FromJson.html},
	note = "[Online; Stand 24. September 2020]"
}
@misc{mlImage,
	author ="MagicLeap",
	title = {JsonUtility.FromJson},
	url = {https://www.magicleap.care/hc/en-us},
	note = "[Online; Stand 1. Oktober 2020]"
}
@misc{mlhardware,
	author ="MagicLeap",
	year="2019",
	title = {JsonUtility.FromJson},
	url = {https://developer.magicleap.com/en-us/learn/guides/magic-leap-one-hardware-specifications},
	note = "[Online; Stand 1. Oktober 2020]"
}

@INPROCEEDINGS{PixelSegmentation,  author={K. {Lien} and B. {Nuernberger} and T. {Höllerer} and M. {Turk}},  booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},   title={PPV: Pixel-Point-Volume Segmentation for Object Referencing in Collaborative Augmented Reality},   year={2016},  volume={},  number={},  pages={77-83},
}%todo

@INPROCEEDINGS{LabelingLanguageLearning,	
	author={B. {Huynh} and J. {Orlosky} and T. {Höllerer}},
	booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
	title={In-Situ Labeling for Augmented Reality Language Learning}, 
	year={2019},
	volume={},
	number={},
	pages={1606-1611},
}

@Inbook{GrundlagenAR,
	author="D{\"o}rner, Ralf
	and Broll, Wolfgang
	and Jung, Bernhard
	and Grimm, Paul
	and G{\"o}bel, Martin",
	editor="D{\"o}rner, Ralf
	and Broll, Wolfgang
	and Grimm, Paul
	and Jung, Bernhard",
	title="Einf{\"u}hrung in Virtual und Augmented Reality",
	bookTitle="Virtual und Augmented Reality (VR/AR): Grundlagen und Methoden der Virtuellen und Augmentierten Realit{\"a}t",
	year="2019",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1--42",
	abstract="Was ist Virtuelle Realit{\"a}t (VR)? Was versteht man unter Augmentierter Realit{\"a}t (AR)? Wozu dienen VR/AR? Welche Grundkonzepte gibt es? Wie sind VR/AR -- Systeme aufgebaut? Wie hat sich VR/AR geschichtlich entwickelt? Diesen Fragen geht das erste Kapitel nach und vermittelt so eine Einf{\"u}hrung in das vorliegende Lehrbuch. Das Kapitel ist grundlegend f{\"u}r das gesamte Buch. Auf ihm bauen alle Folgekapitel auf, w{\"a}hrend alle weiteren Kapitel nicht direkt voneinander abh{\"a}ngen und deswegen in einer Auswahl und Reihenfolge durchgearbeitet werden k{\"o}nnen, die den individuellen Interessen und Bed{\"u}rfnissen der Leser Rechnung tr{\"a}gt. Entsprechende Hinweise, wie dieses Buch effizient von verschiedenen Zielgruppen (Studierende, Lehrende, Anwender, Technologieaffine) genutzt werden kann, finden sich am Ende des Kapitels ebenso wie eine Zusammenfassung, Fragen zur {\"U}berpr{\"u}fung des Gelernten, Empfehlungen f{\"u}r weiterf{\"u}hrende Literatur sowie die im Kapitel verwendeten Referenzen.",
	isbn="978-3-662-58861-1",
	doi="10.1007/978-3-662-58861-1_1",
	url= {https://doi.org/10.1007/978-3-662-58861-1_1}
}


@INPROCEEDINGS{viewmanagement,  author={R. {Azuma} and C. {Furmanski}},  booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},   title={Evaluating label placement for augmented reality view management},   year={2003},  volume={},  number={},  pages={66-75},}

@inproceedings{viewmanagement3d,
	author = {Bell, Blaine and Feiner, Steven and H\"{o}llerer, Tobias},
	title = {View Management for Virtual and Augmented Reality},
	year = {2001},
	isbn = {158113438X},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/502348.502363},
	doi = {10.1145/502348.502363},
	abstract = {We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.},
	booktitle = {Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology},
	pages = {101–110},
	numpages = {10},
	keywords = {annotation, environment management, wearable computing, augmented reality, view management, labeling, virtual environments},
	location = {Orlando, Florida},
	series = {UIST '01}
}

@misc{contextawaremixedreality,
	title={Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction}, 
	author={Long Chen and Wen Tang and Nigel John and Tao Ruan Wan and Jian Jun Zhang},
	year={2018},
	eprint={1803.05541},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@ARTICLE{objectdetection,  author={P. F. {Felzenszwalb} and R. B. {Girshick} and D. {McAllester} and D. {Ramanan}},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Object Detection with Discriminatively Trained Part-Based Models},   year={2010},  volume={32},  number={9},  pages={1627-1645},}

@article{intortodeeplearingmedical,
	title = "A gentle introduction to deep learning in medical image processing",
	journal = "Zeitschrift für Medizinische Physik",
	volume = "29",
	number = "2",
	pages = "86 - 101",
	year = "2019",
	note = "Special Issue: Deep Learning in Medical Physics",
	issn = "0939-3889",
	doi = "https://doi.org/10.1016/j.zemedi.2018.12.003",
	url = "http://www.sciencedirect.com/science/article/pii/S093938891830120X",
	author = "Andreas Maier and Christopher Syben and Tobias Lasser and Christian Riess",
	keywords = "Introduction, Deep learning, Machine learning, Image segmentation, Image registration, Computer-aided diagnosis, Physical simulation, Image reconstruction",
	abstract = "This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modeling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep ()learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future."
}